
---

# Topic 3: Distributed Memory Programming with MPI

## 1. Introduction

This lab transitions from single-core performance to Distributed Memory Programming using the Message Passing Interface (MPI). Multiple processors could be used to solve a single problem, moving from simple "Hello World" broadcasts to complex parallel vector summations.

## 2. Method

### Task 1: MPI Environment Scaling

We tested the basic MPI synchronization by scaling the "Hello World" program from 3 to 6 processors.

* **Observation:** The output order is non-deterministic (Rank 2 might print before Rank 0) because each process operates in its own memory space and execution speed varies slightly across the cluster.

### Task 2: Root-Client Communication Logic

We implemented architecture to perform distributed arithmetic:

* **Root Task (Rank 0):** Acts as the aggregator. It initializes the sum to 0 and enters a loop to `MPI_Recv` values from all other processes.
* **Client Task (Rank > 0):** Performs a local calculation () and uses `MPI_Send` to transmit the result to the Root.

### Task 3: Vector Addition (Serial vs. Parallel)

We compared a standard serial loop against a parallel version. In the parallel version, the workload is distributed across the "Universe Size," and the partial sums are reduced back to the Root.

---

## 3. Mathematical Proof (Task 2)

To verify the accuracy of our MPI communication, we derived a one-operation formula to predict the total sum generated by  processes with input argument .

Every process  (where ) calculates . The total sum is:


Using the arithmetic series formula  where :

$$\text{Total} = \sum_{i=1}^{N-1} (i \times A) = A \times \sum_{i=1}^{N-1} i$$Using the arithmetic series formula $\frac{k(k+1)}{2}$ where $k = N-1$:$$\text{Total} = A \times \frac{(N-1) \times N}{2}$$Verification: For $N=4$ processors and input $A=10$:$$10 \times \frac{4 \times 3}{2} = 10 \times 6 = 60$$
**Verification:** For  processors and input :



*This matches our MPI output perfectly.*

---

## 4. Benchmarking Data: Serial vs. Parallel

The following data tracks the "Wall Clock" (Real) time for summing vectors of increasing sizes.

| Elements () | Serial Time (s) | Parallel (4 Cores) | Parallel (8 Cores) |
| --- | --- | --- | --- |
| **10** | 0.007s | 0.417s | 0.428s |
| **1,000,000** | 0.017s | 0.421s | 0.429s |
| **100,000,000** | 0.703s | 0.603s | 0.541s |
| **1,000,000,000** | 8.328s | 2.458s | 1.426s |

---

## 5. Conclusions

### The Cost of Communication (Overhead)

At small scales the **Serial** code is significantly faster than the Parallel code.

This is because Parallelism introduces "MPI Overhead." The time it takes to initialize the MPI environment and send messages across the network is much greater than the time saved by splitting a small calculation.

### The Ideal value for Parallelism

The benefits of MPI become undeniable at  (1 Billion elements):

* **4-Core Speedup:**  faster than serial.
* **8-Core Speedup:**  faster than serial.

### Efficiency Analysis

While 8 cores are faster than 4, we see diminishing returns. The jump from 1 to 4 cores provided a massive reduction in time (8.3s to 2.4s), but doubling the cores again to 8 only shaved off another second. This is due to the increased complexity of coordinating 8 separate processes and the physical limits of data bandwidth on the `cheetah` cluster.

---

## 6. How to Run

### Compile

```bash
mpicc week_3/vector_parallel.c -o bin/vector_parallel

```

### Run (Example with 8 cores and 1 billion elements)

```bash
time mpirun -np 8 bin/vector_parallel 1000000000

```

---


